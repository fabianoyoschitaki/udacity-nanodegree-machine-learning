{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Using Supervised Classification Algorithms to Predict Bank Term Deposit Subscription\n",
    "Fabiano Shoji Yoschitaki  \n",
    "August 25th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "In order to promote products and services, financial institutions like banks generally run marketing campaigns using two approaches [1]: 1) mass campaigns, which targets general indiscriminate public, broadcasting the same message to different customers and 2) directed marketing, which targets specific contacts, creating a directing relationship to customers.\n",
    "\n",
    "Banks which run marketing campaigns following the first approach have had their campaigns' performance reduced over time, having less than 1% of positive responses [2]. On the other hand, marketing campaigns which follow the second approach have shown better results compared to the first [3]. For this reason, banks are more likely to spend their budget on directed marketing campaigns than on inefficient mass campaigns. \n",
    "\n",
    "The personal reason to work on this domain background comes from the fact that I've worked on a project related to a bank company with the goal to offer the most coherent products to its customers based on their characteristics. I believe that having applied machine learning techniques could have helped us to get better response rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Given the Bank Marketing dataset [4], which is related to direct marketing campaigns of a Portuguese bank institution, a supervised binary classification model has to be created and trained with the objective of predicting whether or not a client will subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "The evaluation metrics that will be considered in this project are: Accuracy and F1-Score. Acuracy is an appropriate metric for supervised classification problems, considering both correct and incorrect classifications in its formula (Figure 1):\n",
    "\n",
    "<img alt=\"Accuracy\" src=\"images/accuracy.png\" width=\"450px\"/>\n",
    "<h4 align=\"center\">Figure 1.1 - Accuracy formula</h4>\n",
    "\n",
    "F1-Score metric, also known as balanced F-score or F-measure, is a metric which can be interpreted as the harmonic average of  both precision and recall metrics (Figure 2). This metric was considered due to the class imbalance that our dataset presents (approximately 88% 'no' vs 12% 'yes).\n",
    "\n",
    "<img alt=\"F1-Score\" src=\"images/f1score.png\" width=\"480px\"/>\n",
    "<h4 align=\"center\">Figure 1.2 - F1-score formula</h4>\n",
    "\n",
    "Precision is the number of TP divided by the number of TP plus the number of FP and recall is the number of TP divided by the number of TP plus the number of FN (Figure 3).\n",
    "\n",
    "<img alt=\"Precision and Recall\" src=\"images/precision_recall.png\" width=\"480px\"/>\n",
    "<h4 align=\"center\">Figure 1.3 - Precision and Recall</h4>\n",
    "\n",
    "Where:\n",
    "\n",
    "- **TP**: True Positive, in our case a person who subscribed a term deposit and is correctly classified.\n",
    "- **TN**: True Negative, in our case a person who didn't subscribe a term deposit and is correctly classified.\n",
    "- **FP**: False Positive, in our case a person who didn't subscribe a term deposit and is incorrectly classified as having subscribed a term deposit.\n",
    "- **FN**: False Negative, in our case a person who subscribed a term deposit and is incorrectly classified as not having subscribed a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "The dataset chosen for this project is related to direct marketing campaigns based on phone calls of a Portuguese banking institution. It was obtained by exploring the University of California Irvine's Machine Learning Repository [5]. The dataset file which will be used is the bank-full.csv and it contains 45211 instances with 17 columns each. The last column is the target label: whether or not the person subscribed a term deposit. The probability for the label 'yes' (did a term deposit) is approximately 12% and for 'no' (didn't do a term deposit) is approximately 88%.\n",
    "The description of the columns follow:\n",
    "\n",
    "- Bank client features:\n",
    "    - **age**: the age of the client (numeric).\n",
    "    - **job**: the type of job of the client (categorical). Possible values: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown'.\n",
    "    - **marital**: the marital status of the client (categorical). Possible values: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed.\n",
    "    - **education**: the education level of the client (categorical). Possible values: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown'.\n",
    "    - **default**: whether or not the client has credit in default (categorical). Possible values: 'no','yes','unknown'.\n",
    "    - **balance**: average yearly balance in Euros (numeric).\n",
    "    - **housing**: whether or not the client has housing loan (categorical). Possible values: 'no','yes','unknown'.\n",
    "    - **loan**: whether or not the client has personal loan (categorical). Possible values: 'no','yes','unknown'.\n",
    "\n",
    "\n",
    "- Features related with the last contact of the current campaign:\n",
    "    - **contact**: contact communication type (categorical). Possible values: 'cellular','telephone'. \n",
    "    - **day**: last contact day of the month (numeric).\n",
    "    - **month**: last contact month of year (categorical). Possible values: 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'.\n",
    "    - **duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "\n",
    "- Other features:\n",
    "    - **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact).\n",
    "    - **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted).\n",
    "    - **previous**: number of contacts performed before this campaign and for this client (numeric).\n",
    "    - **poutcome**: outcome of the previous marketing campaign (categorical). Possible values: 'failure','nonexistent','success'.\n",
    "\n",
    "\n",
    "- Target label:\n",
    "    - **y**: whether or not the client subscribed to a term deposit (categorical). Possible values: 'yes', 'no'. \n",
    "    \n",
    "Displaying the first ten rows of the dataset below (Figure 4):\n",
    "\n",
    "<img alt=\"First 10 rows of the dataset\" src=\"images/first10row.png\"/>\n",
    "<h4 align=\"center\">Figure 2 - Ten first rows of the dataset</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "The Figure 1 shows the imbalance of the target classes 'yes - subscribed' and 'no - didn't subscribed' in our dataset.\n",
    "\n",
    "<img alt=\"Imbalanced Distribution\" src=\"images/distribution_yes_no.png\" width=\"400px\"/>\n",
    "<h4 align=\"center\">Figure 3 - Clients who subscribed to a term deposit vs clients who didn't subscribe</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Age\" src=\"images/distribution_by_age.png\" width=\"600px\"/>\n",
    "<h4 align=\"center\">Figure 4.1 - Clients age distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Age\" src=\"images/distribution_by_age_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 4.2 - Clients age distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_job.png\"/>\n",
    "<h4 align=\"center\">Figure 5.1 - Clients job distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_job_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 5.2 - Clients job distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_marital.png\"/>\n",
    "<h4 align=\"center\">Figure 6.1 - Clients marital status distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_marital_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 6.2 - Clients marital status distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Education\" src=\"images/distribution_by_education.png\"/>\n",
    "<h4 align=\"center\">Figure 7.1 - Clients education distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Education\" src=\"images/distribution_by_education_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 7.2 - Clients education distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Default\" src=\"images/distribution_by_default.png\"/>\n",
    "<h4 align=\"center\">Figure 8.1 - Clients have credit in default distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Default\" src=\"images/distribution_by_default_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 8.2 - Clients have credit in default distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Housing\" src=\"images/distribution_by_housing.png\"/>\n",
    "<h4 align=\"center\">Figure 9.1 - Clients have housing loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Housing\" src=\"images/distribution_by_housing_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 9.2 - Clients have housing loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Personal\" src=\"images/distribution_by_personal.png\"/>\n",
    "<h4 align=\"center\">Figure 10.1 - Clients have personal loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Personal\" src=\"images/distribution_by_personal_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 10.2 - Clients have personal loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Month\" src=\"images/distribution_by_month.png\"/>\n",
    "<h4 align=\"center\">Figure 11.1 - Month of last contact distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Month\" src=\"images/distribution_by_month_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 11.2 - Month of last contact distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Correlation Matrix\" src=\"images/correlation_matrix.png\"/>\n",
    "<h4 align=\"center\">Figure 12 - Correlation Matrix</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"scatter Matrix\" src=\"images/scatter_matrix.png\"/>\n",
    "<h4 align=\"center\">Figure 13 - Scatter Matrix</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Algorithms and Techniques\n",
    "The algorithms that will be used to tackle this binary classification problem are all available in the **scikit-learn** library. As we don't know which algorithm would best fit the data, they were initially used with their default hyper-parameters and trained with the pre-processed training data in order to compare their results and choose the best one (taking into account the accuracy score) for further model tuning.\n",
    "\n",
    "- **Gaussian Naive Bayes (GaussianNB):** Naive Bayes methods are supervised learning algorithms based on Bayes' theorem. Their are called 'naive' because they assume independence between all features. In this project, GaussianNB wasn't initialized with any parameter.\n",
    "\n",
    "- **Decision Trees:** Decision Trees are supervised learning algorithms which can be used both for classification or regression. They predict the target variable by learning from the data simple decisions (e.g. is feature x greater than value y?) in the training process. In this project, DecisionTreeClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Bagging (Ensemble Methods):** Bagging (Bootstrap Aggregating) is an ensemble meta-algorithm that fits base classifiers on random subsets of the original one and eventually aggregate all individual predictions (which can be done by averaging or by voting) yielding a final prediction. In this project, BaggingClassifier was initialized with random_state=1.\n",
    "\n",
    "- **AdaBoost (Ensemble Methods):** Adaboost (Adaptive Boosting) is also an ensemble meta-algorithm that starts fitting a classifier from the original dataset and later fits copies of the same classifier on the same dataset but this time focusing on the incorrect classifications, adjusting the weights on these cases. In this project, AdaBoostClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Random Forest (Ensemble Methods):** Random Forests (or Random Decision Forests) are also an ensemble meta-algorithm that works by fitting various decision trees classifiers on different subsets of the original dataset, averaging the prediction of all decision trees for the final prediction output. In this project, RandomForestClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA):** Linear Discriminant Analysis is a generalization of Fisher's linear discriminant method, used to find a linear combination of features from the dataset that separate two (or more) classes. The fitted model can also be used for dimensionality reduction. In this project, LinearDiscriminantAnalysis wasn't initialized with any parameter.\n",
    "\n",
    "- **K-Nearest Neighbors (KNeighbors):** Neighbors-based classification is a kind of instance-based learning, storing instances of the training data and classifying each point by computing from majority vote of the nearest neighbors of each data point. For instance, if 8 of the 10 nearest data points from data point X is classified as class 'A', then data point X is also going to be classified as class 'A'. In this project, KNeighborsClassifier wasn't initialized with any parameter.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGDC):** Stochastic Gradient Descent is a very efficient approach to discriminative learning of linear classifiers under convex loss functions (such as linear SVM and Logistic Regression). Recently this algorithmn has gained attention in the context of large scale learning. In this project, SGDClassifier wasn't initialized with any parameter.\n",
    "\n",
    "- **Support Vector Machines:** Support Vector Machines are a set of supervosed machine learning algorithms which can be used both for classification or regression purposes. They are commonly used in classification problems and they work by trying to find a hyperplane that best divides a dataset into classes. Support vectors are the nearest data points to the hyperplane, they are critical elements of the data set because removing them would change the position of the hyperplanes. In this project, SVC was initialized with random_state=1.\n",
    "\n",
    "- **Logistic Regression:** Logistic Regression a supervised learning technique borrowed from statistics for binary classification problems. Its name comes from the function used at the core of the algorithm: the logistic function (or sigmoid function). Logistic Regression works by modeling the probability of the 'default' class. In this project, LogisticRegression was initialized with random_state=1.\n",
    "\n",
    "- **eXtreme Gradient Boosting (XGBoost):** XGBoost is an implementation of gradient boosted tree algorithms. It was engineered focusing on efficiency of computing time and memory resources. Boosting is an ensemble method which adds new models in order to correct the errors made by existing models. In this project, XGBClassifier was initialized with random_state=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "The main benchmark considered for this project is a naive predictor that predicts the majority class. In this case, the naive predictor has accuracy of **88.51%** and F1-score of **0.0%**. Also, the best untuned model chosen from the Algorithms and Techniques section will be compared to its tuned version. It is expected that the newly tuned model may overcome the benchmark model accuracy and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "Before training and testing our classification models, the dataset should be prepared and preprocessed. One of the reasons is that it may contain non-numeric features or null values and most of machine learning algorithms expect numbers to perform computations with. \n",
    "\n",
    "In our case, the dataset didn't present null values, so the first task was to apply pandas.get_dummies function in order to convert categorical features into binary variables (e.g. feature **marital** has 3 possible string values: single, married and divorced. After the conversion, feature **marital** was replaced by 3 features: **marital_divorced**, **marital_married** and **marital_single** where possible values are 0 or 1), generating what we call dummy variables.\n",
    "\n",
    "The second preprocessing task was to replace every feature which had either 'yes' or 'no' as possible values to binary values (respectively 1 and 0). After that, the entire dataset was split into features (X - all features but the target) and target (Y - the last column) datasets.\n",
    "\n",
    "With the features and target datasets in hand, these data were shuffled and split into 4:\n",
    " - **X_train:** dataset used for model training with 70% of the original size containing all features but the target.\n",
    " - **y_train:** dataset used for model training with 70% of the original size containing the target variable.\n",
    " - **X_test:** dataset used for model testing with 30% of the original size containing all features but the target.\n",
    " - **y_test:** dataset used for model testing with 30% of the original size containing the target variable.\n",
    " \n",
    "As the last step, X_train and X_test were transformed (normalized) using StandardScaler in order to have mean 0 and a standard deviation 1 before applying the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The process steps implemented by this project are:\n",
    "- **Data Exploration:** in this step some methods and techniques for Exploratory Data Analysis were applied to the dataset. Visualizations of the features were generated, descriptive statistics (mean, standard deviation, min, max, percentiles) were shown and the ratio of clients who subscribed vs clients who didn't subscribe was done. The visualization of the features presented some useful insights (e.g. clients who don't have housing loan are more likely to subscribe to a term deposit). Feature correlation was also presented.\n",
    "\n",
    "- **Data Preparation:** to prepare the data before training the models, null values were checked and the dataset didn't present null values. Then, all the dataset was processed by applying pandas.get_dummies function with the objective to convert all categorical features into binary variables, called dummy variables. After that, all feature values which were either 'yes' or 'no' were replaced by its binary representation (1/0). After all these changes were done, the target feature (the last column of the dataset 'y') was separated from the features, generating 2 datasets: X_all (with the features) and y_all (with the target class). The last step before the model training step was to call train_test_split function to separate 30% of the data in order to keep it as the test dataset.\n",
    "\n",
    "- **Model Selection:** here the selected supervised classification algorithms we chose were initialized (untuned), trained with the training data (70% of the data), tested with the testing data (remaining 30% of the data) and their performance were checked with accuracy and F-1 score metrics. The algorithms used were all from the scikit-learn library: GaussianNB, DecisionTreeClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, LinearDiscriminantAnalysis, KNeighborsClassifier, SGDClassifier, SVC, LogisticRegression, XGBClassifier. The XGBClassifier was chosen as the best untuned model because its accuracy was 90.82% in the test set.\n",
    "\n",
    "- **Model Tuning:** \n",
    "- **Final Evaluation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- _Has an initial solution been found and clearly reported?_\n",
    "- _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- _Are intermediate and final solutions clearly reported as the process is improved?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "- _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- _Can results found from the model be trusted?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "- _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- _Is the final solution significant enough to have solved the problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- _Is the visualization thoroughly analyzed and discussed?_\n",
    "- _If a plot is provided, are the axes, title, and datum clearly defined?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- _Were there any interesting aspects of the project?_\n",
    "- _Were there any difficult aspects of the project?_\n",
    "- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- _If you used your final solution as the new benchmark, do you think an even better solution exists?_\n",
    "\n",
    "-----------\n",
    "\n",
    "**Before submitting, ask yourself. . .**\n",
    "\n",
    "- Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "- Is each section (particularly **Analysis** and **Methodology**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "- Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "- Are all the resources used for this project correctly cited and referenced?\n",
    "- Is the code that implements your solution easily readable and properly commented?\n",
    "- Does the code execute without error and produce results similar to those reported?\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
