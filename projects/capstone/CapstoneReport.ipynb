{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Using Supervised Classification Algorithms to Predict Bank Term Deposit Subscription\n",
    "Fabiano Shoji Yoschitaki  \n",
    "August 25th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "In order to promote products and services, financial institutions like banks generally run marketing campaigns using two approaches [1]: 1) mass campaigns, which targets general indiscriminate public, broadcasting the same message to different customers and 2) directed marketing, which targets specific contacts, creating a directing relationship to customers.\n",
    "\n",
    "Banks which run marketing campaigns following the first approach have had their campaigns' performance reduced over time, having less than 1% of positive responses [2]. On the other hand, marketing campaigns which follow the second approach have shown better results compared to the first [3]. For this reason, banks are more likely to spend their budget on directed marketing campaigns than on inefficient mass campaigns. \n",
    "\n",
    "The personal reason to work on this domain background comes from the fact that I've worked on a project related to a bank company with the goal to offer the most coherent products to its customers based on their characteristics. I believe that having applied machine learning techniques could have helped us to get better response rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Given the Bank Marketing dataset [4], which is related to direct marketing campaigns of a Portuguese bank institution, a supervised binary classification model has to be created and trained with the objective of predicting whether or not a client will subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "The evaluation metrics that will be considered in this project are: Accuracy and F1-Score. Acuracy is an appropriate metric for supervised classification problems, considering both correct and incorrect classifications in its formula (Figure 1):\n",
    "\n",
    "<img alt=\"Accuracy\" src=\"images/accuracy.png\" width=\"450px\"/>\n",
    "<h4 align=\"center\">Figure 1.1 - Accuracy formula</h4>\n",
    "\n",
    "F1-Score metric, also known as balanced F-score or F-measure, is a metric which can be interpreted as the harmonic average of  both precision and recall metrics (Figure 2). This metric was considered due to the class imbalance that our dataset presents (approximately 88% 'no' vs 12% 'yes).\n",
    "\n",
    "<img alt=\"F1-Score\" src=\"images/f1score.png\" width=\"480px\"/>\n",
    "<h4 align=\"center\">Figure 1.2 - F1-score formula</h4>\n",
    "\n",
    "Precision is the number of TP divided by the number of TP plus the number of FP and recall is the number of TP divided by the number of TP plus the number of FN (Figure 3).\n",
    "\n",
    "<img alt=\"Precision and Recall\" src=\"images/precision_recall.png\" width=\"480px\"/>\n",
    "<h4 align=\"center\">Figure 1.3 - Precision and Recall</h4>\n",
    "\n",
    "Where:\n",
    "\n",
    "- **TP**: True Positive, in our case a person who subscribed a term deposit and is correctly classified.\n",
    "- **TN**: True Negative, in our case a person who didn't subscribe a term deposit and is correctly classified.\n",
    "- **FP**: False Positive, in our case a person who didn't subscribe a term deposit and is incorrectly classified as having subscribed a term deposit.\n",
    "- **FN**: False Negative, in our case a person who subscribed a term deposit and is incorrectly classified as not having subscribed a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "The dataset chosen for this project is related to direct marketing campaigns based on phone calls of a Portuguese banking institution. It was obtained by exploring the University of California Irvine's Machine Learning Repository [5]. The dataset file which will be used is the bank-full.csv and it contains 45211 instances with 17 columns each. The last column is the target label: whether or not the person subscribed a term deposit. The probability for the label 'yes' (did a term deposit) is approximately 12% and for 'no' (didn't do a term deposit) is approximately 88%.\n",
    "The description of the columns follow:\n",
    "\n",
    "- Bank client features:\n",
    "    - **age**: the age of the client (numeric).\n",
    "    - **job**: the type of job of the client (categorical). Possible values: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown'.\n",
    "    - **marital**: the marital status of the client (categorical). Possible values: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed.\n",
    "    - **education**: the education level of the client (categorical). Possible values: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown'.\n",
    "    - **default**: whether or not the client has credit in default (categorical). Possible values: 'no','yes','unknown'.\n",
    "    - **balance**: average yearly balance in Euros (numeric).\n",
    "    - **housing**: whether or not the client has housing loan (categorical). Possible values: 'no','yes','unknown'.\n",
    "    - **loan**: whether or not the client has personal loan (categorical). Possible values: 'no','yes','unknown'.\n",
    "\n",
    "\n",
    "- Features related with the last contact of the current campaign:\n",
    "    - **contact**: contact communication type (categorical). Possible values: 'cellular','telephone'. \n",
    "    - **day**: last contact day of the month (numeric).\n",
    "    - **month**: last contact month of year (categorical). Possible values: 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'.\n",
    "    - **duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "\n",
    "- Other features:\n",
    "    - **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact).\n",
    "    - **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted).\n",
    "    - **previous**: number of contacts performed before this campaign and for this client (numeric).\n",
    "    - **poutcome**: outcome of the previous marketing campaign (categorical). Possible values: 'failure','nonexistent','success'.\n",
    "\n",
    "\n",
    "- Target label:\n",
    "    - **y**: whether or not the client subscribed to a term deposit (categorical). Possible values: 'yes', 'no'. \n",
    "    \n",
    "Displaying the first ten rows of the dataset below (Figure 4):\n",
    "\n",
    "<img alt=\"First 10 rows of the dataset\" src=\"images/first10row.png\"/>\n",
    "<h4 align=\"center\">Figure 2 - Ten first rows of the dataset</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "The Figure 1 shows the imbalance of the target classes 'yes - subscribed' and 'no - didn't subscribed' in our dataset. As we can see, the dataset happens to be very imbalanced: clients who subscribed to a term deposit represent only 11.70% of the size.\n",
    "\n",
    "<img alt=\"Imbalanced Distribution\" src=\"images/distribution_yes_no.png\" width=\"400px\"/>\n",
    "<h4 align=\"center\">Figure 3 - Clients who subscribed to a term deposit vs clients who didn't subscribe</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This age histogram shows that most of the clients of the dataset are between the ages of 30 and 40. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Age\" src=\"images/distribution_by_age.png\" width=\"600px\"/>\n",
    "<h4 align=\"center\">Figure 4.1 - Clients age distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that clients between the ages of 60 and 80 tend to subscribe to a term deposit. Few clients at this age interval didn't subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Age\" src=\"images/distribution_by_age_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 4.2 - Clients age distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that blue-collar, management and technician are the most common clients related jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_job.png\"/>\n",
    "<h4 align=\"center\">Figure 5.1 - Clients job distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that clients related to blue-collar jobs tend not to subscribe to a term deposit, while retired and students are more propitious to subscribe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_job_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 5.2 - Clients job distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that married clients are more common in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_marital.png\"/>\n",
    "<h4 align=\"center\">Figure 6.1 - Clients marital status distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that single clients are more propitious to subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_marital_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 6.2 - Clients marital status distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that secondary education level clients are more common in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Education\" src=\"images/distribution_by_education.png\"/>\n",
    "<h4 align=\"center\">Figure 7.1 - Clients education distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that tertiary education level clients are more propitious to subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Education\" src=\"images/distribution_by_education_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 7.2 - Clients education distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that almost all clients don't have credit in default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Default\" src=\"images/distribution_by_default.png\"/>\n",
    "<h4 align=\"center\">Figure 8.1 - Clients have credit in default distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that there's no relevant difference between clients who subscribed vs clients who didn't regarding to whether the client has credit in default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Default\" src=\"images/distribution_by_default_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 8.2 - Clients have credit in default distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that the majority of the clients have a housing loan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Housing\" src=\"images/distribution_by_housing.png\"/>\n",
    "<h4 align=\"center\">Figure 9.1 - Clients have housing loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows clients who don't have a housing loan are more propitious to subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Housing\" src=\"images/distribution_by_housing_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 9.2 - Clients have housing loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that the majority of the clients have a personal loan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Personal\" src=\"images/distribution_by_personal.png\"/>\n",
    "<h4 align=\"center\">Figure 10.1 - Clients have personal loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison image shows that clients who don't have personal loan are slightly more propitious to subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Personal\" src=\"images/distribution_by_personal_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 10.2 - Clients have personal loan distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows that may is the month that the majority of the last contacts with the clients are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Month\" src=\"images/distribution_by_month.png\"/>\n",
    "<h4 align=\"center\">Figure 11.1 - Month of last contact distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Distribution by Month\" src=\"images/distribution_by_month_comparison.png\"/>\n",
    "<h4 align=\"center\">Figure 11.2 - Month of last contact distribution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the feature correlation matrix of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Correlation Matrix\" src=\"images/correlation_matrix.png\"/>\n",
    "<h4 align=\"center\">Figure 12 - Correlation Matrix</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Algorithms and Techniques\n",
    "The algorithms that will be used to tackle this binary classification problem are all available in the **scikit-learn** library. As we don't know which algorithm would best fit the data, they were initially used with their default hyper-parameters and trained with the pre-processed training data in order to compare their results and choose the best one (taking into account the accuracy score) for further model tuning.\n",
    "\n",
    "- **Gaussian Naive Bayes (GaussianNB):** Naive Bayes methods are supervised learning algorithms based on Bayes' theorem. Their are called 'naive' because they assume independence between all features. In this project, GaussianNB wasn't initialized with any parameter.\n",
    "\n",
    "- **Decision Trees:** Decision Trees are supervised learning algorithms which can be used both for classification or regression. They predict the target variable by learning from the data simple decisions (e.g. is feature x greater than value y?) in the training process. In this project, DecisionTreeClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Bagging (Ensemble Methods):** Bagging (Bootstrap Aggregating) is an ensemble meta-algorithm that fits base classifiers on random subsets of the original one and eventually aggregate all individual predictions (which can be done by averaging or by voting) yielding a final prediction. In this project, BaggingClassifier was initialized with random_state=1.\n",
    "\n",
    "- **AdaBoost (Ensemble Methods):** Adaboost (Adaptive Boosting) is also an ensemble meta-algorithm that starts fitting a classifier from the original dataset and later fits copies of the same classifier on the same dataset but this time focusing on the incorrect classifications, adjusting the weights on these cases. In this project, AdaBoostClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Random Forest (Ensemble Methods):** Random Forests (or Random Decision Forests) are also an ensemble meta-algorithm that works by fitting various decision trees classifiers on different subsets of the original dataset, averaging the prediction of all decision trees for the final prediction output. In this project, RandomForestClassifier was initialized with random_state=1.\n",
    "\n",
    "- **Linear Discriminant Analysis (LDA):** Linear Discriminant Analysis is a generalization of Fisher's linear discriminant method, used to find a linear combination of features from the dataset that separate two (or more) classes. The fitted model can also be used for dimensionality reduction. In this project, LinearDiscriminantAnalysis wasn't initialized with any parameter.\n",
    "\n",
    "- **K-Nearest Neighbors (KNeighbors):** Neighbors-based classification is a kind of instance-based learning, storing instances of the training data and classifying each point by computing from majority vote of the nearest neighbors of each data point. For instance, if 8 of the 10 nearest data points from data point X is classified as class 'A', then data point X is also going to be classified as class 'A'. In this project, KNeighborsClassifier wasn't initialized with any parameter.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGDC):** Stochastic Gradient Descent is a very efficient approach to discriminative learning of linear classifiers under convex loss functions (such as linear SVM and Logistic Regression). Recently this algorithmn has gained attention in the context of large scale learning. In this project, SGDClassifier wasn't initialized with any parameter.\n",
    "\n",
    "- **Support Vector Machines:** Support Vector Machines are a set of supervosed machine learning algorithms which can be used both for classification or regression purposes. They are commonly used in classification problems and they work by trying to find a hyperplane that best divides a dataset into classes. Support vectors are the nearest data points to the hyperplane, they are critical elements of the data set because removing them would change the position of the hyperplanes. In this project, SVC was initialized with random_state=1.\n",
    "\n",
    "- **Logistic Regression:** Logistic Regression a supervised learning technique borrowed from statistics for binary classification problems. Its name comes from the function used at the core of the algorithm: the logistic function (or sigmoid function). Logistic Regression works by modeling the probability of the 'default' class. In this project, LogisticRegression was initialized with random_state=1.\n",
    "\n",
    "- **eXtreme Gradient Boosting (XGBoost):** XGBoost is an implementation of gradient boosted tree algorithms. It was engineered focusing on efficiency of computing time and memory resources. Boosting is an ensemble method which adds new models in order to correct the errors made by existing models. In this project, XGBClassifier was initialized with random_state=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "The main benchmark considered for this project is a naive predictor that predicts the majority class. In this case, the naive predictor has accuracy of **88.51%** and F1-score of **0.0%**. Also, the best untuned model chosen from the Algorithms and Techniques section will be compared to its tuned version. It is expected that the newly tuned model may overcome the benchmark model accuracy and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "Before training and testing our classification models, the dataset should be prepared and preprocessed. One of the reasons is that it may contain non-numeric features or null values and most of machine learning algorithms expect numbers to perform computations with. \n",
    "\n",
    "In our case, the dataset didn't present null values, so the first task was to apply pandas.get_dummies function in order to convert categorical features into binary variables (e.g. feature **marital** has 3 possible string values: single, married and divorced. After the conversion, feature **marital** was replaced by 3 features: **marital_divorced**, **marital_married** and **marital_single** where possible values are 0 or 1), generating what we call dummy variables.\n",
    "\n",
    "The second preprocessing task was to replace every feature which had either 'yes' or 'no' as possible values to binary values (respectively 1 and 0). After that, the entire dataset was split into features (X - all features but the target) and target (Y - the last column) datasets.\n",
    "\n",
    "With the features and target datasets in hand, these data were shuffled and split into 4:\n",
    " - **X_train:** dataset used for model training with 70% of the original size containing all features but the target.\n",
    " - **y_train:** dataset used for model training with 70% of the original size containing the target variable.\n",
    " - **X_test:** dataset used for model testing with 30% of the original size containing all features but the target.\n",
    " - **y_test:** dataset used for model testing with 30% of the original size containing the target variable.\n",
    " \n",
    "As the last step, X_train and X_test were transformed (normalized) using StandardScaler in order to have mean 0 and a standard deviation 1 before applying the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The process steps implemented by this project are:\n",
    "- **Data Exploration:** in this step some methods and techniques for Exploratory Data Analysis were applied to the dataset. Visualizations of the features were generated, descriptive statistics (mean, standard deviation, min, max, percentiles) were shown and the ratio of clients who subscribed vs clients who didn't subscribe was done. The visualization of the features presented some useful insights (e.g. clients who don't have housing loan are more likely to subscribe to a term deposit). Feature correlation was also presented.\n",
    "\n",
    "- **Data Preparation:** to prepare the data before training the models, null values were checked and the dataset didn't present null values. Then, all the dataset was processed by applying pandas.get_dummies function with the objective to convert all categorical features into binary variables, called dummy variables. After that, all feature values which were either 'yes' or 'no' were replaced by its binary representation (1/0). After all these changes were done, the target feature (the last column of the dataset 'y') was separated from the features, generating 2 datasets: X_all (with the features) and y_all (with the target class). The last step before the model training step was to call train_test_split function to separate 30% of the data in order to keep it as the test dataset.\n",
    "\n",
    "- **Model Selection:** here the selected supervised classification algorithms we chose were initialized (untuned), trained with the training data (70% of the data), tested with the testing data (remaining 30% of the data) and their performance were checked with accuracy and F-1 score metrics. The algorithms used were all from the scikit-learn library: GaussianNB, DecisionTreeClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, LinearDiscriminantAnalysis, KNeighborsClassifier, SGDClassifier, SVC, LogisticRegression, XGBClassifier.\n",
    "\n",
    "- **Model Tuning:** in the Model Selection step, XGBClassifier was chosen as the best untuned model because its accuracy was 90.82% in the test set. This step is focused on tuning the XGBClassifier. First, we create cross validation sets using shuffled splits with 20% of testing data and 80% training data (10 splits in total). After that, we started to tune the model with grid search technique.\n",
    "\n",
    "- **Final Evaluation:** in the final step, the tuned model is created with its hyperparameters found in the previous tuning step and it is compared to the naive predictor as well as its untuned version for benchmarking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "To refine our untuned XGBoost model, we used the GridSearchCV to search over specified parameter values the best ones. To create the GridSearchCV instance, first, all training data was splitted into 10 subsets, each containing training size 80% and testing size 20%. Also, the scoring parameter was set as a dict mapping the scorer names to the scorer callables (Accuracy and F1-Score).\n",
    "\n",
    "The tuning parameters, range of values tested and the best values found by grid search are shown below:\n",
    "- **scale_pos_weight**: experimented values from 1 to 10. Best value found: 1.\n",
    "- **objective**: experimented values reg:linear, reg:logistic, binary:logistic, binary:logitraw, binary:hinge and count:poisson. Best value found: reg:logistic.\n",
    "- **max_depth** and **min_child_weight**': experimented values from 3 to 10 for max_depth and 1 to 6 for min_child_weight. Best value found for max_depth: 6 and for min_child_weight: 3.\n",
    "- **subsample** and **colsample_bytree**': experimented values from 0.5 to 1.0 (loop: 0.1) for both subsample and colsample_bytree. Best value found for subsample: 1.0 and for colsample_bytree: 0.8.\n",
    "- **reg_alpha** and **reg_lambda**': experimented values from 0 to 5 for reg_alpha and 1 to 6 for reg_lambda. Best value found for reg_alpha: 0 and for reg_lambda: 2.\n",
    "- **gamma**': experimented values from 0.0 to 1.0 (loop: 0.1). Best value: 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "Based on the model selection section, among all algorithms, XGBClassifier presented the best accuracy: 90.82% in the test set. After the model hyperparameters were tuned, its accuracy was increased to **91.25%**. \n",
    "\n",
    "The final tuned model results align with our expectation, we could get a slight better accuracy after tuning and it is higher than the naive predictor, which has accuracy of 88.51%. The final tuned model could generalize well to unseen data because of the train_test_split (80% train/20% test) we did. The accuracy on the training set was **XX** and on the testing set (unseen data) it was **YY**, this makes the model reliable and this way we can trust the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "The results we could get by tuning the XGBClassifier are slightly better than the untuned model and a little better than the naive predictor. One issue I faced in this project was lack of computational processing power, every search for best values for the hyperparameters in the GridSearchCV took a long time or crashed the application. There are probably better values to make the results better, but I had to limit the range for them because of computer resources. Despite the results weren't much higher than the benchmark, I consider the final solution to have solved the problem.\n",
    "\n",
    "**XXXXXXXXXXX TABELA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "For this project, I consider interesting the histograms comparisons of some features I did. For example, comparing the age feature (clients who subscribed to a term deposit vs clients who didn't):\n",
    "<img alt=\"Distribution by Age\" src=\"images/distribution_by_age_comparison.png\"/>\n",
    "\n",
    "This image shows that clients between the ages of 60 and 80 tend to subscribe to a term deposit. Few clients at this age interval didn't subscribe to a term deposit.\n",
    "\n",
    "By comparing the marital status of the clients, we can see that clients who are single also have a greater chance to subscribe to a term deposit.\n",
    "\n",
    "<img alt=\"Distribution by Job\" src=\"images/distribution_by_marital_comparison.png\"/>\n",
    "\n",
    "Another interesting comparison was visualizing clients who have housing loan vs who didn't. The image shows that if the client didn't have a housing loan, chances of subscribing to a term deposit are higher.\n",
    "\n",
    "<img alt=\"Distribution by Housing\" src=\"images/distribution_by_housing_comparison.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "In this project, the biggest challenge for me was to tune the model. After creating a list of classifiers and training, testing and comparing all of them to choose the best one (XGBoost), I spent a lot of time trying to understand its hyperparameters and every search for best values using GridSearchCV took a long time. First, I tried to create a dictionary containing all hyperparameters and specified value ranges, but the processing would not finish. So I started reading how to fine tune the XGBoost in a more concise way.\n",
    "\n",
    "I think the final tuned model fit my expectations, despite knowing that if I had more computational processing power, I'd probably find even better hyperparameter values.\n",
    "\n",
    "I found the project interesting because it has imbalanced data (approximately 88% of 'no' and 12% of 'yes' to a term deposit subscription). This characteristic is very common for almost any product a company wants to sell or provide. Understanding which features are the most relevant and connected to a success of a product is extremely useful for any kind of business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "There is plenty of room for model improvement. I would suggest as further improvements to try longer intervals values for hyperparameters. Also, there are some hyperparameters that I haven't considered for tuning that could show better results. Another improvent I would suggest was to not use accuracy as the main metric. I understood that late by reading about the 'Accuracy Paradox' that may happen in imbalanced data. I would suggest recall rate, AUC/ROC metric. If this final model was used as the new benchmark, it's very likely that a better solution does exist by spending more time on hyperparameter tuning process. \n",
    "\n",
    "-----------\n",
    "#### References\n",
    "\n",
    "[1] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimaraes, Portugal, October, 2011. EUROSIS.\n",
    "\n",
    "[2] Ling, X. and Li, C., 1998. Data Mining for Direct Marketing: Problems and Solutions. In Proceedings of the 4th KDD\n",
    "conference, AAAI Press, 73–79.\n",
    "\n",
    "[3] Ou, C., Liu, C., Huang, J. and Zhong, N. 2003. On Data Mining for Direct Marketing. In Proceedings of the 9th RSFDGrC\n",
    "conference, 2639, 491–498.\n",
    "\n",
    "[4] https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "[5] Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
